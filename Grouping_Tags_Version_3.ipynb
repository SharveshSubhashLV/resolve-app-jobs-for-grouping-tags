{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09712f28-aa00-4bc2-aed2-a2431fceb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "\"\"\" Importing and initiating the model \"\"\"\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "\"\"\" Creating a function for parsing through the \"filtered_phrases.txt\", adding it all to the list and returning the list \"\"\"\n",
    "def parse_phrases_to_list(input_file):\n",
    "    phrases_list = []\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            phrase = line.strip()\n",
    "            if phrase: \n",
    "                phrases_list.append(phrase)\n",
    "    \n",
    "    return phrases_list\n",
    "\n",
    "input_file = 'Filtered_Phrases.txt'\n",
    "\n",
    "\n",
    "phrases = parse_phrases_to_list(input_file)\n",
    "\n",
    "\"\"\" Generating the vector embeddings for all phrases.\"\"\"\n",
    "embeddings = model.encode(phrases, convert_to_tensor=True)\n",
    "\n",
    "\"\"\" Calculating the cosine similarity between all the phrases and saving them as a matrix.\"\"\"\n",
    "cosine_sim_matrix = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "\"\"\" Converting the cosine similarity metrics to cosine distance, so that we can identify the closer ones with their minimum distance.\"\"\"\n",
    "\n",
    "cosine_dist_matrix = 1 - cosine_sim_matrix.numpy()\n",
    "\n",
    "cosine_dist_matrix = np.clip(cosine_dist_matrix, 0, None)\n",
    "\n",
    "db = DBSCAN(metric='precomputed', eps=0.3, min_samples=1)\n",
    "labels = db.fit_predict(cosine_dist_matrix)\n",
    "\n",
    "\"\"\" Selecting the correct canonical phrases, canonical meaning the \"keys\" in our dictionary of tags. \"\"\"\n",
    "unique_labels = set(labels)\n",
    "grouped_phrases = {}\n",
    "\n",
    "for label in unique_labels:\n",
    "    indices = np.where(labels == label)[0]\n",
    "    cluster_embeddings = embeddings[indices]\n",
    "    cluster_embeddings_np = cluster_embeddings.numpy()\n",
    "    centroid = np.mean(cluster_embeddings_np, axis=0)\n",
    "    centroid = torch.mean(cluster_embeddings, axis=0)\n",
    "    distances = cdist([centroid], cluster_embeddings, metric='cosine')[0]\n",
    "    canonical_index = indices[np.argmin(distances)]\n",
    "    canonical_phrase = phrases[canonical_index]\n",
    "    grouped_phrases[canonical_phrase] = [phrases[i] for i in indices]\n",
    "\n",
    "\"\"\" Making sure the canonical phrases are unique\"\"\"\n",
    "canonical_phrases = list(grouped_phrases.keys())\n",
    "canonical_embeddings = model.encode(canonical_phrases, convert_to_tensor=False)\n",
    "canonical_embeddings = np.array(canonical_embeddings)\n",
    "\n",
    "pairwise_sim = util.cos_sim(canonical_embeddings, canonical_embeddings).numpy()\n",
    "\n",
    "uniqueness_threshold = 0.8\n",
    "similar_pairs = []\n",
    "for i, j in itertools.combinations(range(len(canonical_phrases)), 2):\n",
    "    if pairwise_sim[i][j] > uniqueness_threshold:\n",
    "        similar_pairs.append((canonical_phrases[i], canonical_phrases[j]))\n",
    "\n",
    "\"\"\" This is only for handling the similar pairs if we they are found in run time. \"\"\"\n",
    "if similar_pairs:\n",
    "    print(\"Some canonical phrases are too similar:\")\n",
    "    for pair in similar_pairs:\n",
    "        print(f\"- {pair[0]} and {pair[1]}\")\n",
    "else:\n",
    "    print(\"All canonical phrases are semantically unique.\")\n",
    "\n",
    "\n",
    "\"\"\"Outputting the grouped dictionary that we have got.\"\"\"\n",
    "output_file = \"Grouped_Output_bert_and_clustered.txt\"\n",
    "\n",
    "with open(output_file, 'w') as f_out:\n",
    "    for key, phrase in grouped_phrases.items():\n",
    "        f_out.write(f\"{key}: {phrase}\" + '\\n')\n",
    "\n",
    "print(\"Grouped Tags:\")\n",
    "\n",
    "\"\"\"I displayed it to help me in identifying the issue in run time.\"\"\"\n",
    "for canonical, group in grouped_phrases.items():\n",
    "    print(f\"{canonical}: {group}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
